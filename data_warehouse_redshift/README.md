*(Author: Rupali Sharma | https://github.com/SharmaRupali | 2023/09/26)*

# Project: Data Warehouse

## Project Description

Sparkify, a thriving music streaming startup, has experienced significant growth in both its user base and song database. To optimize their data management and analytics capabilities, Sparkify has decided to migrate their data and processes to the cloud. Currently, their data resides in Amazon S3, stored in JSON logs containing user activity on the app, and JSON metadata describing the songs available in their music catalog.

As the designated data engineer for this project, I have been entrusted with the responsibility of designing and implementing an Extract, Transform, Load (ETL) pipeline. This pipeline will facilitate the extraction of data from Amazon S3, staging the data in Amazon Redshift, and transforming it into a set of dimensional tables. These tables will serve as a solid foundation for Sparkify's analytics team to continue extracting valuable insights into user song preferences and behaviors.

## Project Objectives

* Create a robust ETL pipeline that seamlessly extracts data from Amazon S3.
* Establish a staging area in Amazon Redshift for temporary data storage.
* Transform the raw data into a set of well-structured dimensional tables.
* Enable Sparkify's analytics team to conduct in-depth analyses on user activities and song preferences.

## Project Deliverables

* ETL Pipeline:
    * Extraction of data from Amazon S3
    * Staging of data in Amazon Redshift
    * Transformation of data into dimensional tables.

* Dimensional Tables:
    * Fact and dimension tables optimized for analytical queries.

* Documentation
    * Detailed documentation outlining the ETL process, data schema, and pipeline execution.

## Datasets

### Song Dataset

This dataset is a subset of the  [Million Song Dataset](http://millionsongdataset.com/), comprising real data. Each file is structured in JSON format and provides metadata pertaining to individual songs along with details about the respective artists. The files are partitioned by the first three letters of each song's track ID. 

As an illustration, here are the file paths for two specific files within this dataset:
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

*File Location: `s3://udacity-dend/song_data`*

Sample data:
```
{
    "num_songs": 1, 
    "artist_id": "ARJIE2Y1187B994AB7", 
    "artist_latitude": null, 
    "artist_longitude": null, 
    "artist_location": "", 
    "artist_name": "Line Renaud", 
    "song_id": "SOUPIRU12A6D4FA1E1", 
    "title": "Der Kleine Dompfaff", 
    "duration": 152.92036, 
    "year": 0
}
```

### Log Dataset

This dataset encompasses log files in JSON format generated by an [event simulator](https://github.com/Interana/eventsim) derived from the songs dataset mentioned earlier. These log files emulate the activity logs of a hypothetical music streaming application, taking into account various configuration settings. The log files are partitioned by year and month of their creation. 

To illustrate, here are the file paths for two specific files within this dataset:
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

*File Location: `s3://udacity-dend/log_data`*

*Metadata File Location: `s3://udacity-dend/log_json_path.json`*

**Sample Data:**

![Log-Data](images/log-data.png)

## Quickstart

### AWS Resources

To access AWS, I need to set up the following components:
* An IAM User
* An IAM Role with AmazonS3ReadOnlyAccess access policy
* A Redshift Cluster 
* A TCP port for Cluster endpoints

To configure, create, and later delete, the aforementioned AWS resources, please use the script *`create_aws.py`* with respective arguments to the main function depending on if you'd like to `create` or `delete` the resources.
You can also use the `create_vpc` argument to create a new VPC when required and view the *Cluster Endpoint* and *Role ARN* values with `describe`.

The config file *`dwh.cfg`* has already been enriched with all the details required for creating the resources listed above. Please add the *KEY*, *SECRET*, *HOST*, and *ARN* once you have them.

### Database Schema

The database has a ***Star Schema*** with one Fact Table and multiple Dimension Tables. All these tables are derived from two staging tables that are loaded from the aforementioned S3 datasets. Here:

#### Staging Tables
   
* ***staging_events:*** *event data suggesting user actions*
    * artist, auth, first_name, gender, item_in_session, last_name, length, level, location, method, page, registration, session_id, song, status, ts, user_agent, user_id
* ***staging_songs:*** *song data containing song and artist details*
    * num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year

#### Fact Table

* ***songplays:*** *records in event data associated with song plays, i.e., records with page `NextSong`*
    * songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables

* ***users:*** *users in the app*
    * user_id, first_name, last_name, gender, level    
* ***songs:*** *songs in the music database*
    * song_id, title, artist_id, year, duration
* ***artists:*** *artists in the music database*
    * artist_id, name, location, lattitude, longitude
* ***time:*** *timestamps of records in songplays broken down into specific units*
    * start_time, hour, day, week, month, year, weekday

### Process

#### Create AWS resources

I've added a script *`create_aws.py`* that has functions to create new resources and delete the exisitng ones. The resources include:

* An IAM Role with *AmazonS3ReadOnlyAccess* access policy: ***udacity_iam_role***
* A Redshift Cluster with the following specifications:
    * cluster_identifier: ***udacityCluster***
    * cluster_type: ***multi-node***
    * num_nodes: ***4***
    * node_type: ***dc2.large***
    * region: ***us-west-2*** (the same region where the S3 bucket with data is located)
    * db_name: ***udacity_dwh***
    * db_user: ***udacity_user***
    * db_password: ***udacity_Passw0rd***
    * db_port: ***5439***
* Opening up a TCP port for Cluster endpoints: 5439

*Note: Before executing `create_aws.py`, please make sure all the properties required to create an IAM role and a Redshift Cluster are filled in `dwh.cfg`*.

To create the resources, simply run:
```
python3 create_aws.py
```

#### Complete the Configs

After the resources are created, I enrich the config file *`dwh.cfg`* with the details from the same:

* Cluster Endpoint (Host): udacitycluster.cqnivir6xca9.us-west-2.redshift.amazonaws.com
* Role ARN: arn:aws:iam::026608801089:role/udacity_iam_role

#### Create (Recreate) Tables

The script *`sql_queries.py`*, contains all the table defintions. It has the `DROP` statements at the beginnging so it's easier and more automated to recreate the tables when needed.

Next, are the definitions for the **staging** tables for the two data sources we have on S3, followed by the **fact** and **dimension** table definitions, i.e., the `CREATE` statements.

I've chosen the distribution and sorting keys with respect to the ID columns in all the tables with the distribution style *all* for the dimension tables.

To create/recreate the tables, simply run:
```
python3 create_tables.py
```
*Note: The script `sql_queries.py` contains **all** the SQL queries needed for the project. At the end of the file, there are lists, that control which queries will be used in which context that are used by other scripts, i.e., `create_tables.py` and `etl.py`. Logs are printed to the console to track the processes.*

#### Load Data 

* **Into Staging Tables** 

    After creating the tables, we need to load them with the data on S3. The script *`sql_queries.py`* also contains the `COPY` commands for adding all the data from the S3 files to the **staging** tables.

* **Into Fact and Dimension Tables** 

    Then, we need to `INSERT` the data into the **fact** table and the respective **dimension** tables.

To load the data, simply run:
```
python3 etl.py
```
*Note: The script `sql_queries.py` contains **all** the SQL queries needed for the project. At the end of the file, there are lists, that control which queries will be used in which context that are used by other scripts, i.e., `create_tables.py` and `etl.py`. Logs are printed to the console to track the processes.*


### Analysis

Following are a few analytical questions that the corporate users of Sparkify would find interesting:

#### What are the top 5 songs played by users?
```
SELECT s.title AS song_title, a.name AS artist_name, COUNT(sp.songplay_id) AS play_count
FROM songplays sp
    JOIN songs s ON sp.song_id = s.song_id
    JOIN artists a ON sp.artist_id = a.artist_id
GROUP BY song_title, artist_name
ORDER BY play_count DESC
LIMIT 5;
```

**Query Result:**
|   | song_title  | artist_name  |  play_count |
|---|---|---|---|
| 0 | Greece 2000 | 3 Drives On A Vinyl | 55 |
| 1 | You're The One | Dwight Yoakam Duet with Maria McKee | 37 |
| 2 | Stronger | Kanye West | 28 |
| 3 | Revelry | Kings Of Leon | 27 |
| 4 | Somebody To Love | Justin Bieber / Usher | 26|

#### Who are the top 5 artists with the most song plays?

```
SELECT a.name AS artist_name, COUNT(sp.songplay_id) AS play_count
FROM songplays sp
    JOIN artists a ON sp.artist_id = a.artist_id
GROUP BY artist_name
ORDER BY play_count DESC
LIMIT 5;
```

**Query Result:**
|  | artist_name  | play_count  |
|---|---|---|
| 0 | Kings Of Leon | 77 |
| 1 | Coldplay | 71 |
| 2 | The Killers / Toni Halliday | 62 |
| 3 | Eminem / Paul "Bunyan" Rosenburg | 59 |
| 4 |  Kanye West | 56 |

#### How many users are subscribed to each level (free or paid)?

```
SELECT level, COUNT(DISTINCT user_id) AS user_count
FROM users
GROUP BY level;
```

**Query Result:**
|   | level | user_count |
|---|---|---|
| 0 | free | 84 |
| 1 | paid | 23 |

#### What is the average duration of songs played by users in each month?

```
SELECT t.month, AVG(s.duration) AS average_duration
FROM songplays sp
    JOIN songs s ON sp.song_id = s.song_id
    JOIN time t ON sp.start_time = t.start_time
GROUP BY t.month
ORDER BY t.month;
```

**Query Result:**
|   | month |  average_duration |
|---|---|---|
| 0 |    11 |        251.522689 |

#### What is the distribution of song plays across different weekdays?

```
SELECT t.weekday, COUNT(sp.songplay_id) AS play_count
FROM songplays sp
    JOIN time t ON sp.start_time = t.start_time
GROUP BY t.weekday
ORDER BY t.weekday;
```

**Query Result:**
|    | weekday | play_count
|---|---|---|
| 0  | 0 | 424
| 1  | 1 | 1110
| 2  | 2 | 1201
| 3  | 3 | 1540
| 4  | 4 | 1173
| 5  | 5 | 1440
| 6  | 6 | 706

*Note: Queries to retrive these questions have been added to the `sql_queries.py` script.*

## Issues - Findings

* In the song_data, certain anomalies were detected, leading to the adjustment of the artist_location, artist_name, and title columns to accommodate exceptionally long values: varchar(max).
* It took a long time to add the song_data to its staging table.

## Extras

* An IPython notebook *`checks.ipynb`* has been added to the project to perform individual checks for the services or connections if needed.
* A separate script *`analytics.py`* has been added to the project to execute the analytical quries added to *`sql_queries.py`*.